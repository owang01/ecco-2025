{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing ECCO Version 4 Release 4 (Forward Simulation)\n",
    "\n",
    "ECCO Version 4 Release 4 ([V4r4](https://ecco-group.org/products-ECCO-V4r4.htm)) is ECCO's latest publicly available central estimate (see its data repository on [PO.DAAC](https://podaac.jpl.nasa.gov/ECCO?tab=mission-objectives&sections=about%2Bdata), which has been used in numerous studies (e.g., [Wu et al. (2020)](https://www.science.org/doi/10.1126/science.abb9519)). ECCO V4r4 is a **forward** simulation with optimized controls that have been adjusted through an iterative adjoint-based optimization process to minimize the model–data misfit. [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354) provides detailed instructions on how to reproduce the ECCO V4r4 estimate. In this tutorial, we follow those instructions with some modifications tailored for the P-Cluster and reproduce the ECCO V4r4 estimate. \n",
    "\n",
    "## Log in to P-Cluster\n",
    "\n",
    "Users first connect to the P-Cluster and change the directory to the user's directory on /efs_ecco, as described in the [P-Cluster introduction tutorial](../../preliminary/pcluster-login.ipynb):\n",
    "```\n",
    "ssh -i /path/to/privatekey -X USERNAME@34.210.1.198\n",
    "```\n",
    "The directory `/efs_ecco/USERNAME/` (replace `USERNAME` with the user's actual username) is where the run should be conducted. Users can change to that directory with the following command:\n",
    "```\n",
    "cd /efs_ecco/USERNAME/\n",
    "```\n",
    "\n",
    "## Modules \n",
    "Modules on Linux allow users to easily configure their environment for specific software, such as compilers (e.g., GCC, Intel) and MPI libraries (e.g., OpenMPI, MPICH). Users can switch between versions without manually setting environment variables. Running ECCO on different machines and platforms often involves a different set of modules tailored to the system’s architecture and operating system. The modules used in the P-Cluster differ from those specified in Section 5 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354). They have been loaded in the [example .bashrc](example.bashrc), which should have been downloaded and renamed to `/home/USERNAME/.bashrc` as described in the [P-Cluster introduction tutorial](../../preliminary/pcluster-login.ipynb), so that the required modules are loaded automatically. Specificity, the modules loaded in [example .bashrc](example.bashrc) are as follows:\n",
    "\n",
    "| Module Load Command                                                       |\n",
    "|---------------------------------------------------------------------------|\n",
    "| `module load intel-oneapi-compilers-2021.2.0-gcc-11.1.0-adt4bgf`          |\n",
    "| `module load intel-oneapi-mpi-2021.2.0-gcc-11.1.0-ibxno3u`                |\n",
    "| `module load netcdf-c-4.8.1-gcc-11.1.0-6so76nc`                           |\n",
    "| `module load netcdf-fortran-4.5.3-gcc-11.1.0-d35hzyr`                     |\n",
    "| `module load hdf5-1.10.7-gcc-9.4.0-vif4ht3`                               |\n",
    "\n",
    "With these modules pre-loaded by .bashrc, one can skip the module-loading step in the first box of Section 5.1 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354) and proceed directly to the compilation steps in the second box of Section 5.1 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354). \n",
    "\n",
    "## Code, Namelists, and Input Files\n",
    "\n",
    "For the sake of time, the MITgcm code (checkpoint66g), V4-specific code, and V4 namelist files have been downloaded to the P-Cluster at `/efs_ecco/ECCO/V4/r4/`, follwing Sections 2 and 3 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354). Copy the files to the user’s own directory at `/efs_ecco/USERNAME/r4/` using the following command (be sure to replace `USERNAME` with the actual username and maintain the same directory structure as described here):\n",
    "\n",
    "```\n",
    "rsync -av /efs_ecco/ECCO/V4/r4/WORKINGDIR /efs_ecco/USERNAME/r4/\n",
    "```\n",
    "\n",
    "Everyone has a directory at `/efs_ecco/USERNAME/`. There is no need to manually create the subdirectory `/efs_ecco/USERNAME/r4/`; the `rsync` command above will create it automatically.\n",
    "\n",
    "The input files—such as atmospheric forcing and initial conditions, as described in Section 4 of [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354), are several hundreds gigabytes in size. These input files have also been downloaded and stored on the P-Cluster in `/efs_ecco/ECCO/V4/r4/input/`. Do not copy them to the user's own directory. Instead, create a symbolic link in the user's own directory pointing to the input file directory using the following command:\n",
    "```\n",
    "cd /efs_ecco/USERNAME/r4/\n",
    "ln -s /efs_ecco/ECCO/V4/r4/input .\n",
    "```\n",
    "The symbolic link will be used to access the input files in the example run script described below.\n",
    "\n",
    "The directory structure under `/efs_ecco/USERNAME/r4/` now looks like the following:\n",
    "\n",
    "```\n",
    "┌── WORKINGDIR\n",
    "│   ├── ECCO-v4-Configurations\n",
    "│   ├── ECCOV4\n",
    "│   │   └── release4\n",
    "│   │       ├── code\n",
    "│   │       └── namelist\n",
    "│   └── MITgcm\n",
    "└── input\n",
    "```\n",
    "\n",
    "## Compile\n",
    "\n",
    "The steps for compiling the code the same as described in the second box of Section 5.1 of  [Wang et al. (2023)](https://doi.org/10.5281/zenodo.10038354) except for one important change: you need to specify the `optfile` as `../code/linux_ifort_impi_aws_sysmodule`.\n",
    "\n",
    "```\n",
    "cd WORKINGDIR/ECCOV4/release4\n",
    "mkdir build\n",
    "cd build\n",
    "export ROOTDIR=../../../MITgcm\n",
    "../../../MITgcm/tools/genmake2 -mods=../code -optfile=../code/linux_ifort_impi_aws_sysmodule -mpi\n",
    "make depend\n",
    "make all\n",
    "cd ..\n",
    "```\n",
    "\n",
    "The `optfile` `linux_ifort_impi_aws_sysmodule` has been specifically customized for the P-Cluster. If successful, the executable `mitgcmuv` will be generated in the `build` directory.\n",
    "\n",
    "## Run the Model\n",
    "\n",
    "After successfully compiling the code and generating the executable `mitgcmuv` in the `build` directory (`WORKINGDIR/ECCOV4/release5/build/mitgcmuv`), one can proceed with running the model. For this purpose, we provide an [example V4r4 run script](run_script_slurm.bash) that will integrate the model for three months (See below for how to change the [run script](run_script_slurm.bash) to conduct a run over the period of 1992-2017, the entire V4r4 integration period). The example run script is also available on the P-Cluster at `/efs_ecco/ECCO/V4/r4/scripts/run_script_slurm.bash`. \n",
    "\n",
    "### SLURM Directives\n",
    "\n",
    "As described in the [P-Cluster introduction tutorial](../../preliminary/pcluster-login.ipynb), the P-Cluster uses SLURM as the batch system. There are a few SLURM directives at the beginning of the [run script](run_script_slurm.bash) that request the necessary resources for conducting the run. These SLURM directives are as follows:\n",
    "| SBATCH Commands                                | Description                                                                                                            |\n",
    "|------------------------------------------------|------------------------------------------------------------------------------------------------------------------------|\n",
    "| #SBATCH -J ECCOv4r4                            | Job name is ECCOv4r4.                                                                                                  |\n",
    "| #SBATCH --nodes=3                              | Request three nodes.                                                                                                   |\n",
    "| #SBATCH --ntasks-per-node=36                   | Each node has 36 tasks (processes).                                                                                    |\n",
    "| #SBATCH --time=24:00:00                        | Request a wall clock time of 24 hours.                                                                                 |\n",
    "| #SBATCH --exclusive                            | No other jobs will be scheduled on the same nodes while the job is running.                                             |\n",
    "| #SBATCH --partition=sealevel-c5n18xl-demand    | Request an [on-demand](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-on-demand-instances.html) [Amazon EC2 C5n instances](https://aws.amazon.com/ec2/instance-types/c5/) (Tab C5n under Product Details) that has 72 vCPU and 192 GB memory. |\n",
    "| #SBATCH --mem-per-cpu=1GB                      | Each CPU/process/task has 1GB memory.                                                                                  |\n",
    "| #SBATCH -o ECCOv4r4-%j-out                     | Batch output log file.                                                                                                 |\n",
    "| #SBATCH -e ECCOv4r4-%j-out                     | Batch error log file.                                                                                                  |\n",
    "\n",
    "### Submit the Run and Check Job Status\n",
    "\n",
    "Change into the `/efs_ecco/USERNAME/r4/WORKINGDIR/ECCOV4/release4` directory, and copy the run script from `/efs_ecco/ECCO/V4/r4/scripts/run_script_slurm.bash` into this directory (replace `USERNAME` with your actual username, but keep the directory structure the same). Then submit the script using `sbatch` with the following commands:\n",
    "\n",
    "```\n",
    "cd /efs_ecco/USERNAME/r4/WORKINGDIR/ECCOV4/release4\n",
    "cp /efs_ecco/ECCO/V4/r4/scripts/run_script_slurm.bash .\n",
    "sbatch run_script_slurm.bash\n",
    "```\n",
    "\n",
    "Once submitting the job, SLURM will generate a job id and show the following message:\n",
    "```\n",
    "Submitted batch job 123\n",
    "```\n",
    "\n",
    "Users can then check the status the job by using the following command:\n",
    "```\n",
    "squeue\n",
    "```\n",
    "Usually, SLURM takes several minutes to configure a job, with the status (`ST`) showing `CF` (for configuring): \n",
    "```\n",
    "             JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n",
    "               123 sealevel- ECCOv4r4 USERNAME  CF       0:53      3 sealevel-c5n18xl-demand-dy-c5n18xlarge-[1-3]\n",
    "```\n",
    "After a while, squeue will show the status changing to `R` (for run) as shown in following:\n",
    "\n",
    "```\n",
    "             JOBID PARTITION     NAME     USER  ST       TIME  NODES NODELIST(REASON)\n",
    "               123 sealevel- ECCOv4r4 USERNAME  R        3:30      3 sealevel-c5n18xl-demand-dy-c5n18xlarge-[1-3]\n",
    "```\n",
    "The run directory is `/efs_ecco/USERNAME/ECCO/V4/r4/WORKINGDIR/ECCOV4/release4/run/`. The 3-month integration takes less than 20 minutes to complete. `NORMAL END` inside the batch log file `/efs_ecco/USERNAME/ECCO/V4/r4/WORKINGDIR/ECCOV4/release4/ECCOv4r4-123-out` (replace `123` with the actual job ID) indicates a successfully completed run. Another way to check if the run ended normally is to examine the last line of the file `STDOUT.0000` in the run directory. If that line is `PROGRAM MAIN: Execution ended Normally`, then the run completed successfully.\n",
    "\n",
    "The run will output monthly means and snapshots of diagnostic fields in the subdirectory `diags/` of the run directory. These fields can be analyzed using Jupyter Notebooks presented in some of the ECCO Summer School tutorials.\n",
    "\n",
    "\n",
    "To conduct the entire 26-year (1992–2017) run, comment out the following three lines in the [script](run_script_slurm.bash): \n",
    "```\n",
    "unlink data\n",
    "cp -p ../namelist/data .\n",
    "sed -i '/#nTimeSteps=2160,/ s/^#//; /nTimeSteps=227903,/ s/^/#/' data\n",
    "```\n",
    "\n",
    "## References\n",
    "Wang, O., & Fenty, I. (2023). Instructions for reproducing ECCO Version 4 Release 4 (1.5). Zenodo. https://doi.org/10.5281/zenodo.10038354\n",
    "\n",
    "Wu, W., Zhan, Z., Peng, S., Ni, S., & Callies, J. (2020). Seismic ocean thermometry. Science, 1515(6510), 1510–1515. https://doi.org/10.1126/science.abb9519"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
